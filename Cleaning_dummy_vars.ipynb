{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data in order to obtain the dummy variables (used for elastic net and neural network models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B1gnfFZlNMOx",
    "outputId": "cc14f9e6-4ad7-446f-84a2-079af67f81cc"
   },
   "outputs": [],
   "source": [
    "import  numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer, quantile_transform\n",
    "\n",
    "from sklearn.metrics import median_absolute_error, r2_score, mean_squared_error\n",
    "# ....\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "# import lightgbm as lgb \n",
    "\n",
    "from lightgbm import LGBMRegressor,LGBMClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from datetime import datetime\n",
    "\n",
    "# tuner.get_best_hyperparameters(num_trials=1)[0].values\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ClVL1cWNMO0"
   },
   "source": [
    "## Load the data\n",
    "\n",
    "Let's load the data and have a look at it (make sure you copy the folder FFChallenge_v5 in your working directory). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FBdc4X9aNMO1"
   },
   "outputs": [],
   "source": [
    "# background.csv contains 4,242 rows (one per family) and 13,027 columns\n",
    "#                index by challengeID: A unique numeric identifier for each child.\n",
    "#                features: 13,026 background variables asked from birth to age 9, \n",
    "#                which you may use in building your model.\n",
    "background = pd.read_csv('FFChallenge_v5/background.csv', sep=',', header=0,index_col=0,low_memory=False)\n",
    "\n",
    "\n",
    "\n",
    "# train.csv contains 2,121 rows (one per child in the training set) and 7 columns.\n",
    "train = pd.read_csv('FFChallenge_v5/train.csv', sep=',', header=0, index_col=0,low_memory=False)\n",
    "\n",
    "#constantVariables.txt gives the column names of variables that are constant in the data.\n",
    "#We recommend that the first step in any analysis be to remove the variables that are constant.\n",
    "constantVariables = pd.read_csv('FFChallenge_v5/constantVariables.txt',header=None, index_col=0,low_memory=False)\n",
    "\n",
    "\n",
    "\n",
    "########### Holdout dataset for internal testing only\n",
    "test = pd.read_csv('test.csv',header=0, index_col=0,low_memory=False)\n",
    "leaderboard = pd.read_csv('leaderboard.csv', header=0, index_col=0,low_memory=False)\n",
    "leaderboard = leaderboard.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5zvEB-kjNMO4"
   },
   "outputs": [],
   "source": [
    "# Function that maps negative integers from -1 to -9 to NAN (since these are non-response codes)\n",
    "def map_negative_to_nan(cell_number):\n",
    "    non_answers_numbers = list(range(-1,-10,-1))\n",
    "    delta = 0.00001\n",
    "    bool_val = any([True if non_answer_val-delta<cell_number and  non_answer_val+delta>=cell_number else False for non_answer_val in non_answers_numbers])\n",
    "    if bool_val:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return cell_number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apvL-XEDNMO5"
   },
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PecOH1YrNMO8",
    "outputId": "119f6d9e-39e2-421c-814a-b0a5bfeeee6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.074009\n",
      "0:02:55.135191\n"
     ]
    }
   ],
   "source": [
    "# dataframe with dummy variables when someone doesn't answer a question (will be merged back to the background df in a later cleaning step)\n",
    "# Drop constant columns\n",
    "background= background.drop(constantVariables.index.values, axis = 1)\n",
    "\n",
    "numerical_columns = [c for c,v in background.dtypes.iteritems() if v in [np.float,np.int,np.int64]]\n",
    "non_numerical_columns = [c for c,v in background.dtypes.iteritems() if v not in [np.float,np.int,np.int64]]\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "background_copy = background.copy()\n",
    "# Next drop the columns that have no 1s (that means there were no questions that werent answered, so no dummy vars necessary)\n",
    "\n",
    "print(datetime.now()-start)   \n",
    "# It seems like it's much faster to apply the map, then merge everything back together\n",
    "background_copy_numerical = background_copy[numerical_columns].applymap(lambda x:map_negative_to_nan(x))\n",
    "\n",
    "# New backgrounddf with non answers switched to \"NA\"\n",
    "background = background_copy_numerical.merge(background.copy()[non_numerical_columns],how = 'inner', left_index = True, right_index = True)\n",
    "print(datetime.now()-start)\n",
    "\n",
    "# A bunch of new NA values were created, these ones need to be dropped (now that we have the associated dummy variables)\n",
    "mask = (background.isna().sum(axis=0)/len(background)<0.8)\n",
    "background = background[mask[mask].index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eghtvUMUNMO-",
    "outputId": "deeb4ad6-f85a-4ce1-b03c-12ba1ade9fa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categorical columns 4761\n",
      "Number of continuous columns 507\n"
     ]
    }
   ],
   "source": [
    "# Final stats and useful variables associated with each column\n",
    "# Finding numerical, continuous and categorical ccolumns\n",
    "# Columns with more than 20 levels or with non-numerical data are classified as categorical variables\n",
    "\n",
    "numerical_columns = [c for c,v in background.dtypes.iteritems() if v in [np.float,np.int,np.int64]]\n",
    "non_numerical_columns = [c for c,v in background.dtypes.iteritems() if v not in [np.float,np.int,np.int64]]\n",
    "categorical_bools = background.copy().loc[:,(background.apply(pd.Series.nunique) < 20).values]\n",
    "\n",
    "# The categorical columns are the intersection of both the non_numerical columns and the ones that have less than 15 distinct levels\n",
    "categorical_cols_lst = list(set(categorical_bools.columns).union(set(non_numerical_columns)))\n",
    "continuous_cols_lst = list(set(numerical_columns).difference(set(categorical_cols_lst)))\n",
    "\n",
    "print('Number of categorical columns %s' % len(categorical_cols_lst))\n",
    "\n",
    "print('Number of continuous columns %s' % len(continuous_cols_lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUh6vpCzNMO_"
   },
   "source": [
    "### Splitting the data into training and CV set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Rgaown9vNMO_"
   },
   "outputs": [],
   "source": [
    "X_train, X_CV, y_train, y_CV = train_test_split(background.loc[train.index] , train, test_size=0.2 , random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing imputation and obtaining the dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XlUM5d0aNMPA",
    "outputId": "249eebca-9680-48b3-b606-d2177a55d74d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4242, 507)\n",
      "(4242, 5268)\n",
      "(4242, 19729)\n",
      "(4242, 19729)\n",
      "(4242, 19233)\n",
      "(4242, 19028)\n"
     ]
    }
   ],
   "source": [
    "# Get Dummies\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from datetime import datetime\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "\n",
    "# background_test = background.copy()\n",
    "continuous_vars = background[continuous_cols_lst]\n",
    "categorical_vars = background[categorical_cols_lst]\n",
    "# background_test = pd.get_dummies(background_test,columns = categorical_cols_lst, prefix=categorical_cols_lst,drop_first = True)\n",
    "\n",
    "X_train_continuous= continuous_vars.loc[X_train.index.values]\n",
    "X_train_categorical= categorical_vars.loc[X_train.index.values]\n",
    "\n",
    "\n",
    "\n",
    "# Impute continuous data\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean',copy = False)\n",
    "# imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "imputer.fit(X_train_continuous) # Impute on the train set\n",
    "background_imputed_continuous=pd.DataFrame(imputer.transform(continuous_vars)) # Transform the test set using the impute transform from the TRAINING data\n",
    "background_imputed_continuous.columns=continuous_vars.columns\n",
    "background_imputed_continuous.index=continuous_vars.index\n",
    "\n",
    "\n",
    "# Imputing cagtegorical data using mode\n",
    "imp_mode = SimpleImputer(missing_values=np.nan, strategy='most_frequent',copy = False)\n",
    "imp_mode.fit(X_train_categorical) # Impute on the train set\n",
    "background_imputed_categorical=pd.DataFrame(imp_mode.transform(categorical_vars))\n",
    "background_imputed_categorical.columns=categorical_vars.columns\n",
    "background_imputed_categorical.index=categorical_vars.index\n",
    "\n",
    "\n",
    "print(background_imputed_continuous.shape)\n",
    "# Merge the continuous and categorical variables back together\n",
    "background_imputed_tot = background_imputed_continuous.merge(background_imputed_categorical, left_index = True,\n",
    "                                                             right_index=True, how=\"inner\")\n",
    "\n",
    "\n",
    "print(background_imputed_tot.shape)\n",
    "# Create dummy variables now that variables have been filled (mode imputation doesn't work if the variables are already dummy variables)\n",
    "background_imputed_tot = pd.get_dummies(background_imputed_tot,columns = categorical_cols_lst,\n",
    "                                        prefix=categorical_cols_lst)\n",
    "\n",
    "print(background_imputed_tot.shape)\n",
    "\n",
    "\n",
    "# Dropping variables that will not provide any useful information for classification/regression\n",
    "# This includes dummy variables that only appear 1 time in the entire dataset, or 4141 times (all but 1 time)\n",
    "\n",
    "lst_to_drop = list()\n",
    "\n",
    "sums = background_imputed_tot.sum()\n",
    "\n",
    "for col in background_imputed_tot.columns:\n",
    "    # Ignore all continuous variables, since they may unintentionally have a sum that is close to 0\n",
    "    if col not in continuous_cols_lst and (sums.loc[col]<=1 or sums.loc[col]>=len(background_imputed_tot)-1):\n",
    "        lst_to_drop.append(col)\n",
    "\n",
    "# testvvvvv = pd.DataFrame(a)\n",
    "# testvvvvv.sort_values(by=[1])\n",
    "\n",
    "background_imputed_tot = background_imputed_tot.drop(lst_to_drop, axis = 1)\n",
    "print(background_imputed_tot.shape)\n",
    "\n",
    "# Get training values where values never occur\n",
    "X_train = background_imputed_tot.loc[X_train.index.values]\n",
    "\n",
    "\n",
    "# Figuring out if the categorical columns do not appear at all in the training set and dropping these columns\n",
    "lst_to_drop = list()\n",
    "\n",
    "sums = X_train.sum()\n",
    "\n",
    "for col in X_train.columns:\n",
    "    # Ignore all continuous variables, since they may unintentionally have a sum that is close to 0\n",
    "    if col not in continuous_cols_lst and (sums.loc[col]==0 or sums.loc[col]==len(X_train)):\n",
    "        lst_to_drop.append(col)\n",
    "\n",
    "# Drop these columns from the overall set of features\n",
    "background_imputed_tot = background_imputed_tot.drop(lst_to_drop, axis = 1)\n",
    "print(background_imputed_tot.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to pickle files\n",
    "import pickle\n",
    "\n",
    "with open('cleaned_data_dummy_vars.pickle', 'wb') as handle:\n",
    "    pickle.dump(background_imputed_tot, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "X_train = background_imputed_tot.loc[X_train.index.values]\n",
    "with open('X_train_dummy_vars.pickle', 'wb') as handle:\n",
    "    pickle.dump(X_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "X_CV = background_imputed_tot.loc[X_CV.index.values]\n",
    "with open('X_CV_dummy_vars.pickle', 'wb') as handle:\n",
    "    pickle.dump(X_CV, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "x_test = background_imputed_tot.loc[test.index.values]\n",
    "with open('x_test_dummy_vars.pickle', 'wb') as handle:\n",
    "    pickle.dump(x_test, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "x_leaderboard = background_imputed_tot.loc[leaderboard.index.values]\n",
    "with open('x_leaderboard_dummy_vars.pickle', 'wb') as handle:\n",
    "    pickle.dump(x_leaderboard, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('y_train_dummy_vars.pickle', 'wb') as handle:\n",
    "    pickle.dump(y_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('y_CV_dummy_vars.pickle', 'wb') as handle:\n",
    "    pickle.dump(y_CV, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
