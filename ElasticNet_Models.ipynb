{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B1gnfFZlNMOx",
    "outputId": "9e6aa9a1-b06b-4e5a-fb31-59096490b319"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pickle5 in /usr/local/lib/python3.7/dist-packages (0.0.11)\n"
     ]
    }
   ],
   "source": [
    "import  numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer, quantile_transform\n",
    "\n",
    "from sklearn.metrics import median_absolute_error, r2_score, mean_squared_error\n",
    "# ....\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "# import lightgbm as lgb \n",
    "\n",
    "from lightgbm import LGBMRegressor,LGBMClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "from scipy.stats import uniform\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n",
    "from datetime import datetime\n",
    "\n",
    "# tuner.get_best_hyperparameters(num_trials=1)[0].values\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from datetime import datetime\n",
    "!pip3 install pickle5\n",
    "import pickle5 as pickle\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import RepeatedKFold,RepeatedStratifiedKFold\n",
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdRCzcjwo2Ct"
   },
   "outputs": [],
   "source": [
    "# Load cleaned dummy variable data\n",
    "with open('cleaned_data_dummy_vars.pickle', 'rb') as handle:\n",
    "    background_imputed_tot = pickle.load(handle)\n",
    "    \n",
    "with open('X_train_dummy_vars.pickle', 'rb') as handle:\n",
    "    X_train = pickle.load(handle)\n",
    "\n",
    "with open('X_CV_dummy_vars.pickle', 'rb') as handle:\n",
    "    X_CV = pickle.load(handle)\n",
    "\n",
    "with open('x_test_dummy_vars.pickle', 'rb') as handle:\n",
    "    x_test = pickle.load(handle)\n",
    "    \n",
    "with open('x_leaderboard_dummy_vars.pickle', 'rb') as handle:\n",
    "    x_leaderboard = pickle.load(handle)\n",
    "    \n",
    "with open('y_train_dummy_vars.pickle', 'rb') as handle:\n",
    "    y_train = pickle.load(handle)\n",
    "    \n",
    "with open('y_CV_dummy_vars.pickle', 'rb') as handle:\n",
    "    y_CV = pickle.load(handle)\n",
    "    \n",
    "background = pd.read_csv('FFChallenge_v5/background.csv', sep=',', header=0,index_col=0,low_memory=False)\n",
    "# train.csv contains 2,121 rows (one per child in the training set) and 7 columns.\n",
    "train = pd.read_csv('FFChallenge_v5/train.csv', sep=',', header=0, index_col=0,low_memory=False)\n",
    "########### Holdout dataset for internal testing only\n",
    "test = pd.read_csv('test.csv',header=0, index_col=0,low_memory=False)\n",
    "leaderboard = pd.read_csv('leaderboard.csv', header=0, index_col=0,low_memory=False)\n",
    "leaderboard = leaderboard.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XCeuB_KPNMPE",
    "outputId": "0cc67d70-0e72-4b50-9003-ae62371ba177"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of continuous columns 507\n"
     ]
    }
   ],
   "source": [
    "# Final stats and useful variables associated with each column\n",
    "\n",
    "numerical_columns = [c for c,v in background_imputed_tot.dtypes.iteritems() if v in [np.float,np.int,np.int64]]\n",
    "non_numerical_columns = [c for c,v in background_imputed_tot.dtypes.iteritems() if v not in [np.float,np.int,np.int64]]\n",
    "\n",
    "continuous_cols_lst = list()\n",
    "continuous_cols_lst = background_imputed_tot.T.loc[(background_imputed_tot.apply(pd.Series.nunique) >= 15).values==True].index.to_list()\n",
    "print('Number of continuous columns %s' % len(continuous_cols_lst))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFN8ey701-E-"
   },
   "source": [
    "## Training elastic net on continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DqLjI9u1o2Cy",
    "outputId": "aa4d8416-5284-41f0-f7e6-4732fb9750b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------variable: gpa ---------------\n",
      "\n",
      "\n",
      "\n",
      "Fitting 6 folds for each of 44 candidates, totalling 264 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 264 out of 264 | elapsed:  5.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num coefs > 0 = 49\n",
      "---- Train error ----\n",
      "0.328690298399009\n",
      "---- CV error ----\n",
      "240\n",
      "240\n",
      "0.3910418899712505\n",
      "---- leaderboard stats ----\n",
      "0.003792910535217686\n",
      "0.3891345281145492\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Baseline test MSE = 0.42235637187960196\n",
      "Model test MSE = 0.3633380639614692\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------variable: grit ---------------\n",
      "\n",
      "\n",
      "\n",
      "Fitting 6 folds for each of 44 candidates, totalling 264 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 264 out of 264 | elapsed:  6.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num coefs > 0 = 32\n",
      "---- Train error ----\n",
      "0.21564021892159743\n",
      "---- CV error ----\n",
      "281\n",
      "281\n",
      "0.20996893990889384\n",
      "---- leaderboard stats ----\n",
      "0.009504623956038571\n",
      "0.21764914959324488\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Baseline test MSE = 0.25292320173066524\n",
      "Model test MSE = 0.24609645049709947\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------variable: materialHardship ---------------\n",
      "\n",
      "\n",
      "\n",
      "Fitting 6 folds for each of 44 candidates, totalling 264 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 264 out of 264 | elapsed:  4.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num coefs > 0 = 574\n",
      "---- Train error ----\n",
      "0.004149263453137513\n",
      "---- CV error ----\n",
      "288\n",
      "288\n",
      "0.02220667097532756\n",
      "---- leaderboard stats ----\n",
      "0.03239461089498097\n",
      "0.02767916394247585\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Baseline test MSE = 0.024827526011157196\n",
      "Model test MSE = 0.023315472416012787\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_params_elastic_net = dict() # saving best hyperparameters\n",
    "for variable in ['gpa','grit','materialHardship']:\n",
    "    best_p_dict = dict()\n",
    "    print('---------------variable:',variable,'---------------\\n\\n\\n')\n",
    "    # Get non-na rows for this specific variable\n",
    "    y_train_no_na = y_train[variable].dropna()\n",
    "    X_train_no_na = X_train.loc[y_train_no_na.index.values]\n",
    "\n",
    "    y_CV_no_na = y_CV.copy()[variable].dropna()\n",
    "    X_CV_no_na = X_CV.copy().loc[y_CV_no_na.index.values]\n",
    "\n",
    "    mask = leaderboard[variable].isna()\n",
    "    x_leaderboard = background_imputed_tot.loc[leaderboard.index.values]\n",
    "    y_leaderboard = leaderboard[variable]\n",
    "    x_leaderboard_no_na = x_leaderboard.loc[y_leaderboard.index]\n",
    "\n",
    "    # y_train_trans= y_train_no_na.copy()\n",
    "\n",
    "    # Standardizing outcome to be between 0 and 1 and apply the same transformation on the validation and test sets\n",
    "    y_train_mean = y_train_no_na.mean()\n",
    "    y_train_std = y_train_no_na.std()\n",
    "\n",
    "\n",
    "    y_train_trans = (y_train_no_na-y_train_mean)/y_train_std\n",
    "    # y_CV_trans = y_CV_no_na.copy()\n",
    "    # y_CV_trans = (y_CV_no_na-y_train_mean)/y_train_std\n",
    "    y_CV_trans = y_CV_no_na.copy()\n",
    "\n",
    "    # y_leaderboard = (y_leaderboard-y_train_mean)/y_train_std\n",
    "\n",
    "    y_test_trans = test[variable].dropna()\n",
    "    # y_test_trans = (y_test_trans-y_train_mean)/y_train_std\n",
    "    x_test_no_na = x_test.loc[y_test_trans.index.values]\n",
    "\n",
    "\n",
    "\n",
    "    ##### Standardizing features to be between 0 and 1 and apply the same transformation on the validation and test sets\n",
    "    # Pandas applies these operations row-wise\n",
    "    X_train_normalized = X_train_no_na.copy()\n",
    "    X_cv_normalized = X_CV_no_na.copy()\n",
    "    x_leaderboard_normalized = x_leaderboard_no_na.copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ##### Normalize features to be between 0 and 1 and apply the same transformation on the validation and test sets\n",
    "    # Pandas applies these operations row-wise\n",
    "    X_train_standardized = X_train_no_na.copy()\n",
    "    X_cv_standardized = X_CV_no_na.copy()\n",
    "    x_leaderboard_standardized = x_leaderboard_no_na.copy()\n",
    "    x_test_standardized = x_test_no_na.copy()\n",
    "\n",
    "    # Standardising continuous columns to 0 mean, and std of 1\n",
    "    column_mean_normalization = X_train_no_na[continuous_cols_lst].mean()\n",
    "    column_std_normalization = X_train_no_na[continuous_cols_lst].std()\n",
    "\n",
    "\n",
    "    X_train_standardized[continuous_cols_lst]= (X_train_no_na[continuous_cols_lst]-column_mean_normalization)\\\n",
    "    /(column_std_normalization)\n",
    "\n",
    "    X_cv_standardized[continuous_cols_lst]=(X_CV_no_na[continuous_cols_lst]-column_mean_normalization)\\\n",
    "    /(column_std_normalization)\n",
    "\n",
    "    x_leaderboard_standardized[continuous_cols_lst] = (x_leaderboard_no_na[continuous_cols_lst]-column_mean_normalization)\\\n",
    "    /(column_std_normalization)\n",
    "\n",
    "    x_test_standardized[continuous_cols_lst] = (x_test_no_na[continuous_cols_lst]-column_mean_normalization)\\\n",
    "    /(column_std_normalization)\n",
    "\n",
    "\n",
    "    parametersGrid = {\"max_iter\":[10000],\n",
    "                    \"tol\":[0.005],\n",
    "                      \"alpha\": [0.001,0.003, 0.01,0.03,0.1,0.3, 1,3, 10,30, 100],\n",
    "                    \"l1_ratio\": np.arange(0.2, 1.0, 0.2)}\n",
    "\n",
    "    eNet = ElasticNet()\n",
    "    grid = GridSearchCV(eNet, parametersGrid, scoring='neg_mean_squared_error', cv=RepeatedKFold(n_splits=3, n_repeats=2),verbose=1)\n",
    "    grid.fit(X_train_standardized, y_train_no_na)\n",
    "\n",
    "    alpha_val = grid.best_params_['alpha']\n",
    "    l1_ratio_val = grid.best_params_['l1_ratio']\n",
    "\n",
    "    # Only saving alpha and l1 value, since the other parameters are fixed\n",
    "    best_p_dict['alpha'] = alpha_val\n",
    "    best_p_dict['l1_ratio'] = l1_ratio_val\n",
    "    best_params_elastic_net[variable] = best_p_dict # Adding best parameters to the dictionnary of values\n",
    "\n",
    "    model = ElasticNet(alpha=alpha_val,l1_ratio=l1_ratio_val,max_iter=10000,tol=0.005)\n",
    "    model.fit(X_train_standardized, y_train_trans)\n",
    "\n",
    "    # Save best model\n",
    "    joblib.dump(model, 'elastic_net_'+ variable + '.pkl')\n",
    "\n",
    "\n",
    "    coefs = model.coef_\n",
    "    print('num coefs > 0 =', len(coefs[coefs> 0]))\n",
    "\n",
    "\n",
    "    y_pred_train = model.predict(X_train_standardized)\n",
    "    y_pred_all = model.predict(x_leaderboard_standardized)\n",
    "    cv_preds = model.predict(X_cv_standardized)\n",
    "    test_preds = model.predict(x_test_standardized)\n",
    "\n",
    "    # Now remove normalization to accurate comparison to baseline\n",
    "    y_pred_train = (y_pred_train*y_train_std)+y_train_mean\n",
    "    y_pred_all = (y_pred_all*y_train_std)+y_train_mean\n",
    "    cv_preds = (cv_preds*y_train_std)+y_train_mean\n",
    "    test_preds = (test_preds*y_train_std)+y_train_mean\n",
    "\n",
    "\n",
    "    # Can use mean square error as performance metric for both, since for 1 output, the mean square error and brier loss are the same.\n",
    "    print('---- Train error ----')\n",
    "    print(mean_squared_error(y_train_no_na,y_pred_train))\n",
    "    print('---- CV error ----')\n",
    "    print(len(y_CV_trans))\n",
    "    print(len(cv_preds))\n",
    "    print(mean_squared_error(y_CV_trans,cv_preds))\n",
    "    # train_cv_errors[variable] = [mean_squared_error(y_train_trans,y_pred_train),mean_squared_error(y_CV_trans,cv_preds)]\n",
    "\n",
    "    # y_CV_trans = (y_CV_no_na-y_train_mean)/y_train_std\n",
    "    print('---- leaderboard stats ----')\n",
    "    print(r2_score(y_leaderboard[~mask],y_pred_all[~mask]))\n",
    "    print(mean_squared_error(y_leaderboard[~mask],y_pred_all[~mask]))\n",
    "    print('\\n\\n\\n')\n",
    "    preds_baseline_test = np.array([y_test_trans.mean()for i in range(len(y_test_trans))])\n",
    "    print('Baseline test MSE =',mean_squared_error(y_test_trans,preds_baseline_test))\n",
    "\n",
    "    print('Model test MSE =',mean_squared_error(test_preds,y_test_trans))\n",
    "    # acc_nns[variable] = mean_squared_error(y_leaderboard[~mask],y_pred_all[~mask])\n",
    "    print('\\n\\n\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDFQk0Dt2RsM"
   },
   "source": [
    "## Training logistic regression with elastic net regularization on categorical outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "flqcWo_a2Gkd",
    "outputId": "05646443-a3d1-460f-88e1-a3921edb55e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------variable: eviction ---------------\n",
      "\n",
      "\n",
      "\n",
      "Fitting 6 folds for each of 28 candidates, totalling 168 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 168 out of 168 | elapsed: 38.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Train error ----\n",
      "0.049183498870090915\n",
      "---- CV error ----\n",
      "0.018214543052421192\n",
      "---- leaderboard error ----\n",
      "0.05228921042504571\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Baseline test brier loss = 0.0554574230504624\n",
      "Model test brier loss = 0.05339300604782739\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------variable: jobTraining ---------------\n",
      "\n",
      "\n",
      "\n",
      "Fitting 6 folds for each of 28 candidates, totalling 168 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 168 out of 168 | elapsed: 35.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Train error ----\n",
      "0.1606664346772303\n",
      "---- CV error ----\n",
      "0.044938240986095196\n",
      "---- leaderboard error ----\n",
      "0.20188079209608278\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Baseline test brier loss = 0.18521499553665194\n",
      "Model test brier loss = 0.17827193865246235\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------variable: layoff ---------------\n",
      "\n",
      "\n",
      "\n",
      "Fitting 6 folds for each of 28 candidates, totalling 168 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 168 out of 168 | elapsed: 28.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Train error ----\n",
      "0.15483090857016998\n",
      "---- CV error ----\n",
      "0.0380161469958675\n",
      "---- leaderboard error ----\n",
      "0.17493812539951997\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Baseline test brier loss = 0.16721354282637468\n",
      "Model test brier loss = 0.16588094171131537\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for variable in ['eviction','jobTraining','layoff']:\n",
    "    best_p_dict = dict()\n",
    "    print('---------------variable:',variable,'---------------\\n\\n\\n')\n",
    "    # Get non-na rows for this specific variable\n",
    "\n",
    "    y_train_no_na = y_train[variable].dropna()\n",
    "    X_train_no_na = X_train.loc[y_train_no_na.index.values]\n",
    "\n",
    "    mask = leaderboard[variable].isna()\n",
    "    x_leaderboard = background_imputed_tot.loc[leaderboard.index.values]\n",
    "    y_leaderboard = leaderboard[variable]\n",
    "    x_leaderboard_no_na = x_leaderboard.loc[y_leaderboard.index]\n",
    "\n",
    "    y_train_trans= y_train_no_na.copy()\n",
    "\n",
    "    y_CV_trans = y_CV_no_na.copy()\n",
    "\n",
    "    # y_leaderboard = (y_leaderboard-y_train_mean)/y_train_std\n",
    "\n",
    "    y_test_trans = test[variable].dropna()\n",
    "    # y_test_trans = (y_test_trans-y_train_mean)/y_train_std\n",
    "    x_test_no_na = x_test.loc[y_test_trans.index.values]\n",
    "\n",
    "\n",
    "\n",
    "    ##### Standardizing features to be between 0 and 1 and apply the same transformation on the validation and test sets\n",
    "    # Pandas applies these operations row-wise\n",
    "    X_train_normalized = X_train_no_na.copy()\n",
    "    X_cv_normalized = X_CV_no_na.copy()\n",
    "    x_leaderboard_normalized = x_leaderboard_no_na.copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ##### Normalize features to be between 0 and 1 and apply the same transformation on the validation and test sets\n",
    "    # Pandas applies these operations row-wise\n",
    "    X_train_standardized = X_train_no_na.copy()\n",
    "    X_cv_standardized = X_CV_no_na.copy()\n",
    "    x_leaderboard_standardized = x_leaderboard_no_na.copy()\n",
    "    x_test_standardized = x_test_no_na.copy()\n",
    "\n",
    "    # Standardising continuous columns to 0 mean, and std of 1\n",
    "    column_mean_normalization = X_train_no_na[continuous_cols_lst].mean()\n",
    "    column_std_normalization = X_train_no_na[continuous_cols_lst].std()\n",
    "\n",
    "\n",
    "    X_train_standardized[continuous_cols_lst]= (X_train_no_na[continuous_cols_lst]-column_mean_normalization)\\\n",
    "    /(column_std_normalization)\n",
    "\n",
    "    X_cv_standardized[continuous_cols_lst]=(X_CV_no_na[continuous_cols_lst]-column_mean_normalization)\\\n",
    "    /(column_std_normalization)\n",
    "\n",
    "    x_leaderboard_standardized[continuous_cols_lst] = (x_leaderboard_no_na[continuous_cols_lst]-column_mean_normalization)\\\n",
    "    /(column_std_normalization)\n",
    "\n",
    "    x_test_standardized[continuous_cols_lst] = (x_test_no_na[continuous_cols_lst]-column_mean_normalization)\\\n",
    "    /(column_std_normalization)\n",
    "\n",
    "    # weights = y_train_trans.value_counts().to_dict()\n",
    "    # temp = weights[0]\n",
    "    # weights[0] = weights[1]\n",
    "    # weights[1] = temp\n",
    "\n",
    "    parametersGrid = {\"C\": [0.03,0.1,0.3, 1,3, 10,30], # Note: After testing values heiristically, values of C of less than 0.03 lead to models with 0 parameters remaining\n",
    "                    \"l1_ratio\": np.arange(0.2, 1.0, 0.2),\n",
    "                    \"max_iter\":[10000],\n",
    "                    \"solver\" : [\"saga\"],\n",
    "                    \"penalty\" : [\"elasticnet\"],\n",
    "                    \"tol\" : [0.01]}\n",
    "\n",
    "    eNet = LogisticRegression()\n",
    "    grid = GridSearchCV(eNet, parametersGrid, scoring='neg_mean_squared_error', cv=RepeatedStratifiedKFold(3,2),verbose = 1)\n",
    "    grid.fit(X_train_standardized, y_train_no_na)\n",
    "\n",
    "    # train model using best parameters\n",
    "    C_val = grid.best_params_['C']\n",
    "    l1_ratio_val = grid.best_params_['l1_ratio']\n",
    "    best_p_dict['C'] = C_val\n",
    "    best_p_dict['l1_ratio'] = l1_ratio_val\n",
    "    best_params_elastic_net[variable] = best_p_dict # Adding best parameters to the dictionnary of values\n",
    "\n",
    "\n",
    "    model = LogisticRegression(penalty = 'elasticnet', solver = 'saga',C = 0.02, l1_ratio = l1_ratio_val,max_iter=1000,tol = 0.01)\n",
    "    model.fit(X_train_standardized, y_train_trans)\n",
    "\n",
    "    joblib.dump(model, 'elastic_net_'+ variable + '.pkl')\n",
    "\n",
    "    # coefs = model.coef_\n",
    "    # print('num coefs > 0 =', len(coefs[coefs> 0]))\n",
    "\n",
    "\n",
    "    y_pred_train = model.predict_proba(X_train_standardized)[:,1]\n",
    "    y_pred_all = model.predict_proba(x_leaderboard_standardized)[:,1]\n",
    "    cv_preds = model.predict_proba(X_cv_standardized)[:,1]\n",
    "    test_preds = model.predict_proba(x_test_standardized)[:,1]\n",
    "\n",
    "\n",
    "    # Can use mean square error as performance metric for both, since for 1 output, the mean square error and brier loss are the same.\n",
    "    print('---- Train error ----')\n",
    "    print(mean_squared_error(y_train_no_na,y_pred_train))\n",
    "    print('---- CV error ----')\n",
    "    # print(len(y_CV_trans))\n",
    "    # print(len(cv_preds))\n",
    "    print(mean_squared_error(y_CV_trans,cv_preds))\n",
    "    # train_cv_errors[variable] = [mean_squared_error(y_train_trans,y_pred_train),mean_squared_error(y_CV_trans,cv_preds)]\n",
    "\n",
    "    # y_CV_trans = (y_CV_no_na-y_train_mean)/y_train_std\n",
    "    print('---- leaderboard error ----')\n",
    "    print(mean_squared_error(y_leaderboard[~mask],y_pred_all[~mask]))\n",
    "    print('\\n\\n\\n')\n",
    "    preds_baseline_test = np.array([y_test_trans.mean()for i in range(len(y_test_trans))])\n",
    "    print('Baseline test brier loss =',mean_squared_error(y_test_trans,preds_baseline_test))\n",
    "\n",
    "    print('Model test brier loss =',mean_squared_error(test_preds,y_test_trans))\n",
    "    # acc_nns[variable] = mean_squared_error(y_leaderboard[~mask],y_pred_all[~mask])\n",
    "    print('\\n\\n\\n\\n')\n",
    "\n",
    "# Save best parameters to pickle file\n",
    "with open('best_params_elastic_net.pickle', 'wb') as handle:\n",
    "    pickle.dump(best_params_elastic_net, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Copy of ElasticNet_Models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
