{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data in order to obtain the Label encoded variables (used for tree-based models)\n",
    "Note the code is very similar to the cleaning_dummy_vars file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer, quantile_transform\n",
    "\n",
    "from sklearn.metrics import median_absolute_error, r2_score, mean_squared_error\n",
    "# ....\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "# import lightgbm as lgb \n",
    "\n",
    "from lightgbm import LGBMRegressor,LGBMClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing \n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# background.csv contains 4,242 rows (one per family) and 13,027 columns\n",
    "#                index by challengeID: A unique numeric identifier for each child.\n",
    "#                features: 13,026 background variables asked from birth to age 9, \n",
    "#                which you may use in building your model.\n",
    "background = pd.read_csv('FFChallenge_v5/background.csv', sep=',', header=0,index_col=0,low_memory=False)\n",
    "\n",
    "\n",
    "\n",
    "# train.csv contains 2,121 rows (one per child in the training set) and 7 columns.\n",
    "train = pd.read_csv('FFChallenge_v5/train.csv', sep=',', header=0, index_col=0,low_memory=False)\n",
    "\n",
    "#constantVariables.txt gives the column names of variables that are constant in the data.\n",
    "#We recommend that the first step in any analysis be to remove the variables that are constant.\n",
    "constantVariables = pd.read_csv('FFChallenge_v5/constantVariables.txt',header=None, index_col=0,low_memory=False)\n",
    "\n",
    "\n",
    "\n",
    "########### Holdout dataset for internal testing only\n",
    "test = pd.read_csv('test.csv',header=0, index_col=0,low_memory=False)\n",
    "leaderboard = pd.read_csv('leaderboard.csv', header=0, index_col=0,low_memory=False)\n",
    "leaderboard = leaderboard.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that maps negative integers from -1 to -9 to NAN (since these are non-response codes)\n",
    "def map_negative_to_nan(cell_number):\n",
    "    non_answers_numbers = list(range(-1,-10,-1))\n",
    "    delta = 0.00001\n",
    "    bool_val = any([True if non_answer_val-delta<cell_number and  non_answer_val+delta>=cell_number else False for non_answer_val in non_answers_numbers])\n",
    "    if bool_val:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return cell_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop constant columns\n",
    "background= background.drop(constantVariables.index.values, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.082007\n",
      "0:02:58.842327\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "numerical_columns = [c for c,v in background.dtypes.iteritems() if v in [np.float,np.int,np.int64]]\n",
    "non_numerical_columns = [c for c,v in background.dtypes.iteritems() if v not in [np.float,np.int,np.int64]]\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "background_copy = background.copy()\n",
    "\n",
    "print(datetime.now()-start)   \n",
    "# It seems like it's much faster to apply the map, then merge everything back together\n",
    "background_copy_numerical = background_copy[numerical_columns].applymap(lambda x:map_negative_to_nan(x))\n",
    "\n",
    "# New backgrounddf with non answers switched to \"NA\"\n",
    "background = background_copy_numerical.merge(background.copy()[non_numerical_columns],how = 'inner', left_index = True, right_index = True)\n",
    "print(datetime.now()-start)\n",
    "\n",
    "# A bunch of new NA values were created, these ones need to be dropped (now that we have the associated dummy variables)\n",
    "mask = (background.isna().sum(axis=0)/len(background)<0.8)\n",
    "background = background[mask[mask].index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categorical columns 4761\n",
      "Number of continuous columns 507\n"
     ]
    }
   ],
   "source": [
    "# Final stats and useful variables associated with each column\n",
    "# Finding numerical, continuous and categorical ccolumns\n",
    "# Columns with more than 20 levels or with non-numerical data are classified as categorical variables\n",
    "numerical_columns = [c for c,v in background.dtypes.iteritems() if v in [np.float,np.int,np.int64]]\n",
    "non_numerical_columns = [c for c,v in background.dtypes.iteritems() if v not in [np.float,np.int,np.int64]]\n",
    "categorical_bools = background.copy().loc[:,(background.apply(pd.Series.nunique) < 20).values]\n",
    "\n",
    "# The categorical columns are the intersection of both the non_numerical columns and the ones that have less than 20 distinct levels\n",
    "categorical_cols_lst = list(set(categorical_bools.columns).union(set(non_numerical_columns)))\n",
    "continuous_cols_lst = list(set(numerical_columns).difference(set(categorical_cols_lst)))\n",
    "\n",
    "\n",
    "print('Number of categorical columns %s' % len(categorical_cols_lst))\n",
    "\n",
    "print('Number of continuous columns %s' % len(continuous_cols_lst))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into training and CV set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_CV, y_train, y_CV = train_test_split(background.loc[train.index] , train, test_size=0.2 , random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing imputation and obtaining the label endoded variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4242, 507)\n",
      "(4242, 5268)\n",
      "0:00:12.138398\n"
     ]
    }
   ],
   "source": [
    "# Get label encodings\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from datetime import datetime\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "continuous_vars = background[continuous_cols_lst]\n",
    "categorical_vars = background[categorical_cols_lst]\n",
    "\n",
    "X_train_continuous= continuous_vars.loc[X_train.index.values]\n",
    "X_train_categorical= categorical_vars.loc[X_train.index.values]\n",
    "\n",
    "\n",
    "# Impute continuous data\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean',copy = False)\n",
    "# imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "imputer.fit(X_train_continuous) # Impute on the train set\n",
    "background_imputed_continuous=pd.DataFrame(imputer.transform(continuous_vars)) # Transform the test set using the impute transform from the TRAINING data\n",
    "background_imputed_continuous.columns=continuous_vars.columns\n",
    "background_imputed_continuous.index=continuous_vars.index\n",
    "\n",
    "\n",
    "# Imputing cagtegorical data using mode\n",
    "imp_mode = SimpleImputer(missing_values=np.nan, strategy='most_frequent',copy = False)\n",
    "imp_mode.fit(X_train_categorical) # Impute on the train set\n",
    "background_imputed_categorical=pd.DataFrame(imp_mode.transform(categorical_vars))\n",
    "background_imputed_categorical.columns=categorical_vars.columns\n",
    "background_imputed_categorical.index=categorical_vars.index\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "background_imputed_categorical = background_imputed_categorical.apply(le.fit_transform)\n",
    "\n",
    "# Merge the continuous and categorical variables back together\n",
    "background_imputed_tot = background_imputed_continuous.merge(background_imputed_categorical, left_index = True,\n",
    "                                                             right_index=True, how=\"inner\")\n",
    "\n",
    "\n",
    "\n",
    "print(background_imputed_continuous.shape)\n",
    "# Merge the continuous and categorical variables back together\n",
    "background_imputed_tot = background_imputed_continuous.merge(background_imputed_categorical, left_index = True,\n",
    "                                                             right_index=True, how=\"inner\")\n",
    "\n",
    "\n",
    "print(background_imputed_tot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to pickle files\n",
    "import pickle\n",
    "\n",
    "with open('cleaned_data_label_encoding.pickle', 'wb') as handle:\n",
    "    pickle.dump(background_imputed_tot, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "X_train = background_imputed_tot.loc[X_train.index.values]\n",
    "with open('X_train_label_encoding.pickle', 'wb') as handle:\n",
    "    pickle.dump(X_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "X_CV = background_imputed_tot.loc[X_CV.index.values]\n",
    "with open('X_CV_label_encoding.pickle', 'wb') as handle:\n",
    "    pickle.dump(X_CV, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "x_test = background_imputed_tot.loc[test.index.values]\n",
    "with open('x_test_label_encoding.pickle', 'wb') as handle:\n",
    "    pickle.dump(x_test, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "x_leaderboard = background_imputed_tot.loc[leaderboard.index.values]\n",
    "with open('x_leaderboard_label_encoding.pickle', 'wb') as handle:\n",
    "    pickle.dump(x_leaderboard, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('y_train_label_encoding.pickle', 'wb') as handle:\n",
    "    pickle.dump(y_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('y_CV_label_encoding.pickle', 'wb') as handle:\n",
    "    pickle.dump(y_CV, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
