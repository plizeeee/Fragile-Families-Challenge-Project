{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer, quantile_transform\n",
    "\n",
    "from sklearn.metrics import median_absolute_error, r2_score, mean_squared_error\n",
    "# ....\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "# import lightgbm as lgb \n",
    "\n",
    "from lightgbm import LGBMRegressor,LGBMClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from datetime import datetime\n",
    "\n",
    "# tuner.get_best_hyperparameters(num_trials=1)[0].values\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from datetime import datetime\n",
    "import pickle5 as pickle\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dummy variable features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXGjd5Tdjh4H"
   },
   "outputs": [],
   "source": [
    "# Load cleaned dummy variable data\n",
    "with open('cleaned_data_dummy_vars.pickle', 'rb') as handle:\n",
    "    background_imputed_tot = pickle.load(handle)\n",
    "    \n",
    "with open('X_train_dummy_vars.pickle', 'rb') as handle:\n",
    "    X_train = pickle.load(handle)\n",
    "\n",
    "with open('X_CV_dummy_vars.pickle', 'rb') as handle:\n",
    "    X_CV = pickle.load(handle)\n",
    "\n",
    "with open('x_test_dummy_vars.pickle', 'rb') as handle:\n",
    "    x_test = pickle.load(handle)\n",
    "    \n",
    "with open('x_leaderboard_dummy_vars.pickle', 'rb') as handle:\n",
    "    x_leaderboard = pickle.load(handle)\n",
    "    \n",
    "with open('y_train_dummy_vars.pickle', 'rb') as handle:\n",
    "    y_train = pickle.load(handle)\n",
    "    \n",
    "with open('y_CV_dummy_vars.pickle', 'rb') as handle:\n",
    "    y_CV = pickle.load(handle)\n",
    "    \n",
    "background = pd.read_csv('FFChallenge_v5/background.csv', sep=',', header=0,index_col=0,low_memory=False)\n",
    "# train.csv contains 2,121 rows (one per child in the training set) and 7 columns.\n",
    "train = pd.read_csv('FFChallenge_v5/train.csv', sep=',', header=0, index_col=0,low_memory=False)\n",
    "########### Holdout dataset for internal testing only\n",
    "test = pd.read_csv('test.csv',header=0, index_col=0,low_memory=False)\n",
    "leaderboard = pd.read_csv('leaderboard.csv', header=0, index_col=0,low_memory=False)\n",
    "leaderboard = leaderboard.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2_b917_HkpEr",
    "outputId": "a7beb860-3a9f-43e4-998d-7d0547fcc5fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of continuous columns 507\n"
     ]
    }
   ],
   "source": [
    "# Final stats and useful variables associated with each column\n",
    "\n",
    "numerical_columns = [c for c,v in background_imputed_tot.dtypes.iteritems() if v in [np.float,np.int,np.int64]]\n",
    "non_numerical_columns = [c for c,v in background_imputed_tot.dtypes.iteritems() if v not in [np.float,np.int,np.int64]]\n",
    "continuous_cols_lst = list()\n",
    "continuous_cols_lst = background_imputed_tot.T.loc[(background_imputed_tot.apply(pd.Series.nunique) >= 15).values==True].index.to_list()\n",
    "\n",
    "print('Number of continuous columns %s' % len(continuous_cols_lst))\n",
    "# background = background[numerical_columns]\n",
    "# background.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ClVL1cWNMO0"
   },
   "source": [
    "## Creating regression neural network model for continuous outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WNICYPXMcifb"
   },
   "outputs": [],
   "source": [
    "# Creating a neural network with 1 hidden layer, where the three hyperparameters to be tunred are \"num_hidden_layer_1\",\"dropout_layer_1\" and the learning_rate\n",
    "# Uses Adam optimizer and mean squared error loss\n",
    "# Uses a linear layer for the final layer for continuous predictions\n",
    "def make_regression_nn(num_hidden_layer_1,dropout_layer_1,learning_rate):\n",
    "\n",
    "    model = Sequential()\n",
    "    # Defining input layer, with size of input feature (size of image is 784)\n",
    "    model.add(Dense(num_hidden_layer_1, input_shape=(len(X_train_normalized.columns),)))\n",
    "    model.add(Activation('sigmoid'))     \n",
    "    # Using a dropout rate of 20% for regularization for each layer\n",
    "    model.add(Dropout(dropout_layer_1))\n",
    "\n",
    "    \n",
    "\n",
    "    # Defining third hidden layer\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer=Adam(\n",
    "            learning_rate = learning_rate),\n",
    "            loss='mean_squared_error', metrics=[MeanSquaredError()])#     model_test.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VWZ9niI444tB",
    "outputId": "a2c3f187-a18d-42bd-e232-2fae24780c3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------variable: gpa ---------------\n",
      "\n",
      "\n",
      "\n",
      "Gridsearch complete\n",
      "best params from gridsearch = {'dropout_layer_1': 0.75, 'learning_rate': 0.0005, 'num_hidden_layer_1': 1024}\n",
      "Epoch 1/500\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.77314, saving model to best_model_nngpa.h5\n",
      "Epoch 2/500\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.77314 to 0.45609, saving model to best_model_nngpa.h5\n",
      "Epoch 3/500\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.45609 to 0.42667, saving model to best_model_nngpa.h5\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.42667\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.42667 to 0.38143, saving model to best_model_nngpa.h5\n",
      "Epoch 6/500\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.38143 to 0.37913, saving model to best_model_nngpa.h5\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.37913 to 0.37835, saving model to best_model_nngpa.h5\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.37835\n",
      "Epoch 9/500\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.37835\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.37835 to 0.36697, saving model to best_model_nngpa.h5\n",
      "Epoch 11/500\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.36697\n",
      "Epoch 12/500\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.36697\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.36697\n",
      "Epoch 14/500\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.36697\n",
      "Epoch 15/500\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.36697\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.36697\n",
      "Epoch 17/500\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.36697 to 0.36249, saving model to best_model_nngpa.h5\n",
      "Epoch 18/500\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.36249\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.36249 to 0.36219, saving model to best_model_nngpa.h5\n",
      "Epoch 20/500\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.36219\n",
      "Epoch 21/500\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.36219\n",
      "Epoch 22/500\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.36219\n",
      "Epoch 23/500\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.36219\n",
      "Epoch 24/500\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.36219 to 0.36190, saving model to best_model_nngpa.h5\n",
      "Epoch 25/500\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.36190\n",
      "Epoch 26/500\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.36190\n",
      "Epoch 27/500\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.36190\n",
      "Epoch 28/500\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.36190\n",
      "Epoch 29/500\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.36190\n",
      "Epoch 30/500\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.36190\n",
      "Epoch 31/500\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.36190\n",
      "Epoch 32/500\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.36190\n",
      "Epoch 33/500\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.36190\n",
      "Epoch 34/500\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.36190\n",
      "Epoch 35/500\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.36190\n",
      "Epoch 36/500\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.36190\n",
      "Epoch 37/500\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.36190\n",
      "Epoch 38/500\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.36190\n",
      "Epoch 39/500\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.36190\n",
      "Epoch 40/500\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.36190\n",
      "Epoch 41/500\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.36190\n",
      "Epoch 42/500\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.36190\n",
      "Epoch 43/500\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.36190\n",
      "Epoch 44/500\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.36190\n",
      "Epoch 45/500\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.36190\n",
      "Epoch 46/500\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.36190\n",
      "Epoch 47/500\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.36190\n",
      "Epoch 48/500\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.36190\n",
      "Epoch 49/500\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.36190\n",
      "Epoch 00049: early stopping\n",
      "---- Train error ----\n",
      "0.22660614652205635\n",
      "---- CV error ----\n",
      "0.3704092535927579\n",
      "---- leaderboard stats ----\n",
      "0.007581316378873493\n",
      "0.38765471579854827\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Baseline test MSE = 0.42235637187960196\n",
      "Model test MSE = 0.35829776981549316\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------variable: grit ---------------\n",
      "\n",
      "\n",
      "\n",
      "Gridsearch complete\n",
      "best params from gridsearch = {'dropout_layer_1': 0.85, 'learning_rate': 0.002, 'num_hidden_layer_1': 1024}\n",
      "Epoch 1/500\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 6.21339, saving model to best_model_nngrit.h5\n",
      "Epoch 2/500\n",
      "\n",
      "Epoch 00002: val_loss improved from 6.21339 to 0.51728, saving model to best_model_nngrit.h5\n",
      "Epoch 3/500\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.51728 to 0.23227, saving model to best_model_nngrit.h5\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.23227 to 0.21792, saving model to best_model_nngrit.h5\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.21792\n",
      "Epoch 6/500\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.21792\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.21792\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.21792\n",
      "Epoch 9/500\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.21792\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.21792 to 0.21738, saving model to best_model_nngrit.h5\n",
      "Epoch 11/500\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.21738\n",
      "Epoch 12/500\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.21738\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.21738\n",
      "Epoch 14/500\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.21738\n",
      "Epoch 15/500\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.21738\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.21738\n",
      "Epoch 17/500\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.21738\n",
      "Epoch 18/500\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.21738\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.21738\n",
      "Epoch 20/500\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.21738\n",
      "Epoch 21/500\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.21738\n",
      "Epoch 22/500\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.21738\n",
      "Epoch 23/500\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.21738\n",
      "Epoch 24/500\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.21738\n",
      "Epoch 25/500\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.21738\n",
      "Epoch 26/500\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.21738\n",
      "Epoch 27/500\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.21738\n",
      "Epoch 28/500\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.21738\n",
      "Epoch 29/500\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.21738\n",
      "Epoch 30/500\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.21738\n",
      "Epoch 31/500\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.21738\n",
      "Epoch 32/500\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.21738\n",
      "Epoch 33/500\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.21738 to 0.21705, saving model to best_model_nngrit.h5\n",
      "Epoch 34/500\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.21705\n",
      "Epoch 35/500\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.21705\n",
      "Epoch 36/500\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.21705\n",
      "Epoch 37/500\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.21705\n",
      "Epoch 38/500\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.21705\n",
      "Epoch 39/500\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.21705\n",
      "Epoch 40/500\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.21705\n",
      "Epoch 41/500\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.21705\n",
      "Epoch 42/500\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.21705\n",
      "Epoch 43/500\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.21705\n",
      "Epoch 44/500\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.21705\n",
      "Epoch 45/500\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.21705\n",
      "Epoch 46/500\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.21705\n",
      "Epoch 47/500\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.21705\n",
      "Epoch 48/500\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.21705\n",
      "Epoch 49/500\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.21705\n",
      "Epoch 50/500\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.21705\n",
      "Epoch 51/500\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.21705\n",
      "Epoch 52/500\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.21705\n",
      "Epoch 53/500\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.21705\n",
      "Epoch 54/500\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.21705\n",
      "Epoch 55/500\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.21705\n",
      "Epoch 56/500\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.21705\n",
      "Epoch 57/500\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.21705\n",
      "Epoch 58/500\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.21705\n",
      "Epoch 00058: early stopping\n",
      "---- Train error ----\n",
      "0.24126516379799784\n",
      "---- CV error ----\n",
      "0.2128699192627016\n",
      "---- leaderboard stats ----\n",
      "-0.003492459960729999\n",
      "0.22050509857604073\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Baseline test MSE = 0.25292320173066524\n",
      "Model test MSE = 0.2532482265775962\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------variable: materialHardship ---------------\n",
      "\n",
      "\n",
      "\n",
      "Gridsearch complete\n",
      "best params from gridsearch = {'dropout_layer_1': 0.75, 'learning_rate': 0.0001, 'num_hidden_layer_1': 1024}\n",
      "Epoch 1/500\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.66358, saving model to best_model_nnmaterialHardship.h5\n",
      "Epoch 2/500\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.66358 to 0.09666, saving model to best_model_nnmaterialHardship.h5\n",
      "Epoch 3/500\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.09666 to 0.02800, saving model to best_model_nnmaterialHardship.h5\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02800 to 0.02390, saving model to best_model_nnmaterialHardship.h5\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.02390\n",
      "Epoch 6/500\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02390 to 0.02215, saving model to best_model_nnmaterialHardship.h5\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02215 to 0.02011, saving model to best_model_nnmaterialHardship.h5\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02011 to 0.01959, saving model to best_model_nnmaterialHardship.h5\n",
      "Epoch 9/500\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.01959\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01959 to 0.01880, saving model to best_model_nnmaterialHardship.h5\n",
      "Epoch 11/500\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01880 to 0.01864, saving model to best_model_nnmaterialHardship.h5\n",
      "Epoch 12/500\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.01864\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01864 to 0.01847, saving model to best_model_nnmaterialHardship.h5\n",
      "Epoch 14/500\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01847 to 0.01847, saving model to best_model_nnmaterialHardship.h5\n",
      "Epoch 15/500\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.01847\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.01847\n",
      "Epoch 17/500\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01847 to 0.01825, saving model to best_model_nnmaterialHardship.h5\n",
      "Epoch 18/500\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.01825\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.01825\n",
      "Epoch 20/500\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01825 to 0.01809, saving model to best_model_nnmaterialHardship.h5\n",
      "Epoch 21/500\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01809 to 0.01808, saving model to best_model_nnmaterialHardship.h5\n",
      "Epoch 22/500\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.01808 to 0.01795, saving model to best_model_nnmaterialHardship.h5\n",
      "Epoch 23/500\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.01795\n",
      "Epoch 24/500\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.01795 to 0.01787, saving model to best_model_nnmaterialHardship.h5\n",
      "Epoch 25/500\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.01787\n",
      "Epoch 26/500\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.01787 to 0.01785, saving model to best_model_nnmaterialHardship.h5\n",
      "Epoch 27/500\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.01785 to 0.01776, saving model to best_model_nnmaterialHardship.h5\n",
      "Epoch 28/500\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.01776\n",
      "Epoch 29/500\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.01776\n",
      "Epoch 30/500\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.01776\n",
      "Epoch 31/500\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.01776 to 0.01756, saving model to best_model_nnmaterialHardship.h5\n",
      "Epoch 32/500\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.01756\n",
      "Epoch 33/500\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.01756\n",
      "Epoch 34/500\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.01756\n",
      "Epoch 35/500\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.01756\n",
      "Epoch 36/500\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.01756\n",
      "Epoch 37/500\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.01756\n",
      "Epoch 38/500\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.01756\n",
      "Epoch 39/500\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.01756\n",
      "Epoch 40/500\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.01756\n",
      "Epoch 41/500\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.01756 to 0.01752, saving model to best_model_nnmaterialHardship.h5\n",
      "Epoch 42/500\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.01752\n",
      "Epoch 43/500\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.01752\n",
      "Epoch 44/500\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.01752 to 0.01743, saving model to best_model_nnmaterialHardship.h5\n",
      "Epoch 45/500\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.01743\n",
      "Epoch 46/500\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.01743 to 0.01743, saving model to best_model_nnmaterialHardship.h5\n",
      "Epoch 47/500\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.01743\n",
      "Epoch 48/500\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.01743\n",
      "Epoch 49/500\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.01743\n",
      "Epoch 50/500\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.01743\n",
      "Epoch 51/500\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.01743\n",
      "Epoch 52/500\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.01743\n",
      "Epoch 53/500\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.01743\n",
      "Epoch 54/500\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.01743 to 0.01738, saving model to best_model_nnmaterialHardship.h5\n",
      "Epoch 55/500\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.01738\n",
      "Epoch 56/500\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.01738 to 0.01729, saving model to best_model_nnmaterialHardship.h5\n",
      "Epoch 57/500\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.01729 to 0.01720, saving model to best_model_nnmaterialHardship.h5\n",
      "Epoch 58/500\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.01720\n",
      "Epoch 59/500\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.01720 to 0.01717, saving model to best_model_nnmaterialHardship.h5\n",
      "Epoch 60/500\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.01717\n",
      "Epoch 61/500\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.01717\n",
      "Epoch 62/500\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.01717\n",
      "Epoch 63/500\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.01717 to 0.01712, saving model to best_model_nnmaterialHardship.h5\n",
      "Epoch 64/500\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.01712\n",
      "Epoch 65/500\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.01712\n",
      "Epoch 66/500\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.01712 to 0.01710, saving model to best_model_nnmaterialHardship.h5\n",
      "Epoch 67/500\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.01710\n",
      "Epoch 68/500\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.01710\n",
      "Epoch 69/500\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.01710\n",
      "Epoch 70/500\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.01710\n",
      "Epoch 71/500\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.01710\n",
      "Epoch 72/500\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.01710\n",
      "Epoch 73/500\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.01710\n",
      "Epoch 74/500\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.01710\n",
      "Epoch 75/500\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.01710 to 0.01693, saving model to best_model_nnmaterialHardship.h5\n",
      "Epoch 76/500\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.01693\n",
      "Epoch 77/500\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.01693\n",
      "Epoch 78/500\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.01693\n",
      "Epoch 79/500\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.01693\n",
      "Epoch 80/500\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.01693\n",
      "Epoch 81/500\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.01693\n",
      "Epoch 82/500\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.01693\n",
      "Epoch 83/500\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.01693\n",
      "Epoch 84/500\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.01693\n",
      "Epoch 85/500\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.01693\n",
      "Epoch 86/500\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.01693\n",
      "Epoch 87/500\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.01693\n",
      "Epoch 88/500\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.01693\n",
      "Epoch 89/500\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.01693\n",
      "Epoch 90/500\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.01693\n",
      "Epoch 91/500\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.01693\n",
      "Epoch 92/500\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.01693\n",
      "Epoch 93/500\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.01693\n",
      "Epoch 94/500\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.01693\n",
      "Epoch 95/500\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.01693\n",
      "Epoch 96/500\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.01693\n",
      "Epoch 97/500\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.01693\n",
      "Epoch 98/500\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.01693\n",
      "Epoch 99/500\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.01693\n",
      "Epoch 100/500\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.01693\n",
      "Epoch 00100: early stopping\n",
      "---- Train error ----\n",
      "0.00430887121689471\n",
      "---- CV error ----\n",
      "0.01798380189807966\n",
      "---- leaderboard stats ----\n",
      "0.0836370407846585\n",
      "0.026213331244872642\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Baseline test MSE = 0.024827526011157196\n",
      "Model test MSE = 0.021285872484274722\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# variable = 'jobTraining'\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from keras.layers.core import Lambda\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.wrappers.scikit_learn import KerasClassifier,KerasRegressor\n",
    "from tensorflow.keras.metrics import MeanSquaredError\n",
    "from keras.regularizers import l1\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "# from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RepeatedKFold,RepeatedStratifiedKFold\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(0)\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(0)\n",
    "\n",
    "\n",
    "acc_nns = dict()\n",
    "best_params_dict = dict()\n",
    "train_cv_errors = dict()\n",
    "history_accs = dict()\n",
    "\n",
    "for variable in ['gpa', 'grit', 'materialHardship']: # Looping through continuous outcomes\n",
    "    print('---------------variable:',variable,'---------------\\n\\n\\n')\n",
    "    # Get non-na rows for this specific variable\n",
    "    y_train_no_na = y_train[variable].dropna()\n",
    "    X_train_no_na = X_train.loc[y_train_no_na.index.values]\n",
    "\n",
    "    y_CV_no_na = y_CV.copy()[variable].dropna()\n",
    "    X_CV_no_na = X_CV.copy().loc[y_CV_no_na.index.values]\n",
    "\n",
    "    mask = leaderboard[variable].isna()\n",
    "    x_leaderboard = background_imputed_tot.loc[leaderboard.index.values]\n",
    "    y_leaderboard = leaderboard[variable]\n",
    "    x_leaderboard_no_na = x_leaderboard.loc[y_leaderboard.index]\n",
    "\n",
    "    y_train_trans= y_train_no_na.copy()\n",
    "    y_CV_trans = y_CV_no_na.copy()\n",
    "    y_test_trans = test[variable].dropna()\n",
    "\n",
    "\n",
    "    x_test_no_na = x_test.loc[y_test_trans.index.values]\n",
    "\n",
    "\n",
    "\n",
    "    ##### Normalize features to be between 0 and 1 and apply the same transformation on the validation and test sets\n",
    "    # Pandas applies these operations row-wise\n",
    "    X_train_normalized = X_train_no_na.copy()\n",
    "    X_cv_normalized = X_CV_no_na.copy()\n",
    "    x_leaderboard_normalized = x_leaderboard_no_na.copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ##### Normalize features to be between 0 and 1 and apply the same transformation on the validation and test sets\n",
    "    # Pandas applies these operations row-wise\n",
    "    X_train_normalized = X_train_no_na.copy()\n",
    "    X_cv_normalized = X_CV_no_na.copy()\n",
    "    x_leaderboard_normalized = x_leaderboard_no_na.copy()\n",
    "    x_test_normalized = x_test_no_na.copy()\n",
    "\n",
    "\n",
    "    # Getting normalization parameters\n",
    "    column_min_normalization = X_train_no_na[continuous_cols_lst].min()\n",
    "    column_max_normalization = X_train_no_na[continuous_cols_lst].max()\n",
    "\n",
    "    # Normalizing features\n",
    "    X_train_normalized[continuous_cols_lst]=(X_train_no_na[continuous_cols_lst]-column_min_normalization)\\\n",
    "    /(column_max_normalization-column_min_normalization)\n",
    "\n",
    "    X_cv_normalized[continuous_cols_lst]=(X_CV_no_na[continuous_cols_lst]-column_min_normalization)\\\n",
    "    /(column_max_normalization-column_min_normalization)\n",
    "\n",
    "    x_leaderboard_normalized[continuous_cols_lst] = (x_leaderboard_no_na[continuous_cols_lst]-column_min_normalization)\\\n",
    "    /(column_max_normalization-column_min_normalization)\n",
    "\n",
    "    x_test_normalized[continuous_cols_lst] = (x_test_no_na[continuous_cols_lst]-column_min_normalization)\\\n",
    "    /(column_max_normalization-column_min_normalization)\n",
    "\n",
    "    classifier = KerasRegressor(make_regression_nn, epochs=500,verbose = 0)\n",
    "\n",
    "    # Parameter search space (need high values for dropout to reduce overfitting)\n",
    "    params = [{'num_hidden_layer_1': [64,128,256,512,1024],\n",
    "            'dropout_layer_1': [0.75,0.8,0.85,0.9,0.95],\n",
    "            'learning_rate': [0.002,0.0005,0.0001]}]\n",
    "\n",
    "    # Use grid search with repeated k fold cross validation\n",
    "    grid = GridSearchCV(classifier,\n",
    "                          param_grid=params,\n",
    "                          scoring='neg_mean_squared_error', #sklearn optimizing by maximizing negative brier score\n",
    "                          #  n_jobs=-1,\n",
    "                          verbose=0,\n",
    "                          cv=RepeatedKFold(n_splits=3, n_repeats=2)     # Number of folds for CV\n",
    "                          #  fit_params={'callbacks': [tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                          #                                          min_delta=0.001, \n",
    "                          #                                          patience=25, \n",
    "                          #                                          mode='min',\n",
    "                          #                                          verbose=2)],\n",
    "                          #             'validation_data': (np.array(X_cv_normalized), np.array(y_CV_trans))\n",
    "                          #             }\n",
    "                    )\n",
    "\n",
    "    grid.fit(np.array(X_train_normalized), np.array(y_train_trans),callbacks= [tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                                  min_delta=0.001, \n",
    "                                                                  patience=25, \n",
    "                                                                  mode='min',\n",
    "                                                                  verbose=0)],validation_data= (np.array(X_cv_normalized), np.array(y_CV_trans)))\n",
    "\n",
    "    errs =grid.cv_results_['mean_test_score']\n",
    "    print('Gridsearch complete')\n",
    "    \n",
    "    # Obtain the best parameters and save them\n",
    "    best_params = grid.best_params_\n",
    "    print('best params from gridsearch =',best_params)\n",
    "    best_params_dict[variable] = best_params\n",
    "    # model_test = make_model(best_params['num_hidden_layer_1'],best_params['dropout_layer_1'],best_params['learning_rate'])\n",
    "\n",
    "    num_hidden_layer_1 = best_params['num_hidden_layer_1']\n",
    "    dropout_layer_1 =best_params['dropout_layer_1']\n",
    "\n",
    "    learning_rate = best_params['learning_rate'] # jobTraining lr (also works well for layoff)\n",
    "    # learning_rate = 0.000060947\n",
    "\n",
    "    \n",
    "    ###### Train the same neural network architecture from scratch (doesn't seem to work when I call my make regression function, so made it manually here)\n",
    "    model_test = Sequential()\n",
    "    # Defining input layer, with size of input feature (size of image is 784)\n",
    "    model_test.add(Dense(num_hidden_layer_1, input_shape=(len(X_train_normalized.columns),)))\n",
    "    model_test.add(Activation('sigmoid'))     \n",
    "    # Using a dropout rate of 20% for regularization for each layer\n",
    "    model_test.add(Dropout(dropout_layer_1))\n",
    "\n",
    "    # Defining output layer\n",
    "    model_test.add(Dense(1))\n",
    "\n",
    "    model_test.compile(optimizer=Adam(\n",
    "              learning_rate = learning_rate),\n",
    "              loss='mean_squared_error', metrics=[MeanSquaredError()])#     model_test.summary()\n",
    "\n",
    "\n",
    "    # Use early stopping, and save the model performance to be able to load the network parameters 25 iterations before early stopping if it was initiated\n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                                  min_delta=0.0001, \n",
    "                                                                  patience=25, \n",
    "                                                                  verbose=2)\n",
    "    # Saving parameters at the earliest iteration when validation loss was at a minimum\n",
    "    mc = ModelCheckpoint('best_model_nn' + variable + '.h5', monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "    history = model_test.fit(X_train_normalized, y_train_trans, epochs=500, validation_split=0.2,\\\n",
    "                    validation_data=(X_cv_normalized, y_CV_trans),\\\n",
    "                      callbacks=[stop_early, mc], verbose = -1\n",
    "                    )\n",
    "\n",
    "    history_accs[variable] = history.history\n",
    "\n",
    "    model_test = load_model('best_model_nn' + variable + '.h5')\n",
    "\n",
    "\n",
    "    y_pred_train = model_test.predict(X_train_normalized)\n",
    "    y_pred_all = model_test.predict(x_leaderboard_normalized)\n",
    "    cv_preds = model_test.predict(X_cv_normalized)\n",
    "    test_preds = model_test.predict(x_test_normalized)\n",
    "\n",
    "    # Can use mean square error as performance metric for both, since for 1 output, the mean square error and brier loss are the same.\n",
    "    print('---- Train error ----')\n",
    "    print(mean_squared_error(y_train_trans,y_pred_train))\n",
    "    print('---- CV error ----')\n",
    "    print(mean_squared_error(y_CV_trans,cv_preds))\n",
    "    train_cv_errors[variable] = [mean_squared_error(y_train_trans,y_pred_train),mean_squared_error(y_CV_trans,cv_preds)]\n",
    "\n",
    "    # y_CV_trans = (y_CV_no_na-y_train_mean)/y_train_std\n",
    "    print('---- leaderboard stats ----')\n",
    "    print(r2_score(y_leaderboard[~mask],y_pred_all[~mask]))\n",
    "    print(mean_squared_error(y_leaderboard[~mask],y_pred_all[~mask]))\n",
    "    print('\\n\\n\\n')\n",
    "    preds_baseline_test = np.array([y_test_trans.mean()for i in range(len(y_test_trans))])\n",
    "    print('Baseline test MSE =',mean_squared_error(y_test_trans,preds_baseline_test))\n",
    "\n",
    "    print('Model test MSE =',mean_squared_error(test_preds,y_test_trans))\n",
    "    # acc_nns[variable] = mean_squared_error(y_leaderboard[~mask],y_pred_all[~mask])\n",
    "    print('\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating classification neural network model for categorical outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8a9quCDBpMO"
   },
   "outputs": [],
   "source": [
    "# Creating a neural network with 1 hidden layer, where the three hyperparameters to be tunred are \"num_hidden_layer_1\",\"dropout_layer_1\" and the learning_rate\n",
    "# Uses Adam optimizer and binary cross entropy loss\n",
    "# Uses a sigmoid activation for the final layer for binary predictions\n",
    "def make_classification_nn(num_hidden_layer_1,dropout_layer_1,learning_rate):\n",
    "    model_test = Sequential()\n",
    "    # Defining input layer, with size of input feature (size of image is 784)\n",
    "    model_test.add(Dense(num_hidden_layer_1, input_shape=(len(X_train_normalized.columns),)))\n",
    "    model_test.add(Activation('sigmoid'))     \n",
    "    # Using a dropout rate of 20% for regularization for each layer\n",
    "    model_test.add(Dropout(dropout_layer_1))\n",
    "\n",
    "    \n",
    "\n",
    "    # Defining third hidden layer\n",
    "    model_test.add(Dense(1))\n",
    "    model_test.add(Activation('sigmoid'))\n",
    "\n",
    "    model_test.compile(optimizer=Adam(\n",
    "            learning_rate = learning_rate),\n",
    "            loss='binary_crossentropy', metrics=['accuracy',MeanSquaredError()])#     model_test.summary()\n",
    "    \n",
    "    return model_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q3mMY9dScl8R",
    "outputId": "b023550c-d9ae-4f95-9db7-882b26900071"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------variable: eviction ---------------\n",
      "\n",
      "\n",
      "\n",
      "Gridsearch complete\n",
      "best params from gridsearch = {'dropout_layer_1': 0.9, 'learning_rate': 2.5e-05, 'num_hidden_layer_1': 64}\n",
      "Epoch 1/500\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.23335, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 2/500\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.23335\n",
      "Epoch 3/500\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.23335 to 0.22964, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.22964 to 0.22728, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.22728 to 0.22627, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 6/500\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.22627 to 0.22407, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.22407\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.22407 to 0.22399, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 9/500\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.22399\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.22399\n",
      "Epoch 11/500\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.22399 to 0.22307, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 12/500\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.22307 to 0.22275, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.22275 to 0.22245, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 14/500\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.22245 to 0.22215, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 15/500\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.22215\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.22215 to 0.22210, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 17/500\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.22210 to 0.22112, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 18/500\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.22112\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.22112\n",
      "Epoch 20/500\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.22112\n",
      "Epoch 21/500\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.22112 to 0.22044, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 22/500\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.22044\n",
      "Epoch 23/500\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.22044 to 0.22003, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 24/500\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.22003\n",
      "Epoch 25/500\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.22003 to 0.21976, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 26/500\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.21976 to 0.21971, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 27/500\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.21971\n",
      "Epoch 28/500\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.21971\n",
      "Epoch 29/500\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.21971 to 0.21951, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 30/500\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.21951 to 0.21946, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 31/500\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.21946 to 0.21841, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 32/500\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.21841 to 0.21804, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 33/500\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.21804 to 0.21787, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 34/500\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.21787 to 0.21738, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 35/500\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.21738 to 0.21696, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 36/500\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.21696 to 0.21695, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 37/500\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.21695 to 0.21656, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 38/500\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.21656\n",
      "Epoch 39/500\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.21656\n",
      "Epoch 40/500\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.21656 to 0.21631, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 41/500\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.21631 to 0.21609, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 42/500\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.21609\n",
      "Epoch 43/500\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.21609 to 0.21587, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 44/500\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.21587 to 0.21551, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 45/500\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.21551 to 0.21545, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 46/500\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.21545\n",
      "Epoch 47/500\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.21545\n",
      "Epoch 48/500\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.21545 to 0.21518, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 49/500\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.21518\n",
      "Epoch 50/500\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.21518 to 0.21486, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 51/500\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.21486\n",
      "Epoch 52/500\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.21486 to 0.21481, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 53/500\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.21481 to 0.21457, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 54/500\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.21457\n",
      "Epoch 55/500\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.21457\n",
      "Epoch 56/500\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.21457\n",
      "Epoch 57/500\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.21457\n",
      "Epoch 58/500\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.21457\n",
      "Epoch 59/500\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.21457 to 0.21382, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 60/500\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.21382\n",
      "Epoch 61/500\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.21382\n",
      "Epoch 62/500\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.21382 to 0.21358, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 63/500\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.21358\n",
      "Epoch 64/500\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.21358\n",
      "Epoch 65/500\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.21358\n",
      "Epoch 66/500\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.21358\n",
      "Epoch 67/500\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.21358\n",
      "Epoch 68/500\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.21358 to 0.21335, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 69/500\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.21335\n",
      "Epoch 70/500\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.21335\n",
      "Epoch 71/500\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.21335\n",
      "Epoch 72/500\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.21335\n",
      "Epoch 73/500\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.21335\n",
      "Epoch 74/500\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.21335 to 0.21275, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 75/500\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.21275\n",
      "Epoch 76/500\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.21275\n",
      "Epoch 77/500\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.21275 to 0.21236, saving model to best_model_nnh2eviction.h5\n",
      "Epoch 78/500\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.21236\n",
      "Epoch 79/500\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.21236\n",
      "Epoch 80/500\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.21236\n",
      "Epoch 81/500\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.21236\n",
      "Epoch 82/500\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.21236\n",
      "Epoch 83/500\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.21236\n",
      "Epoch 84/500\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.21236\n",
      "Epoch 85/500\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.21236\n",
      "Epoch 86/500\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.21236\n",
      "Epoch 87/500\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.21236\n",
      "Epoch 88/500\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.21236\n",
      "Epoch 89/500\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.21236\n",
      "Epoch 90/500\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.21236\n",
      "Epoch 91/500\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.21236\n",
      "Epoch 92/500\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.21236\n",
      "Epoch 93/500\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.21236\n",
      "Epoch 94/500\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.21236\n",
      "Epoch 95/500\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.21236\n",
      "Epoch 96/500\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.21236\n",
      "Epoch 97/500\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.21236\n",
      "Epoch 98/500\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.21236\n",
      "Epoch 99/500\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.21236\n",
      "Epoch 100/500\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.21236\n",
      "Epoch 101/500\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.21236\n",
      "Epoch 102/500\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.21236\n",
      "Epoch 00102: early stopping\n",
      "---- Train error ----\n",
      "0.04050011679133834\n",
      "---- CV error ----\n",
      "0.04792624911338727\n",
      "---- leaderboard stats ----\n",
      "0.016123687616420135\n",
      "0.05253878492614346\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Baseline test brier score = 0.0554574230504624\n",
      "Model test brier score = 0.054133745322839294\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------variable: jobTraining ---------------\n",
      "\n",
      "\n",
      "\n",
      "Gridsearch complete\n",
      "best params from gridsearch = {'dropout_layer_1': 0.95, 'learning_rate': 2.5e-05, 'num_hidden_layer_1': 256}\n",
      "Epoch 1/500\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.62834, saving model to best_model_nnh2jobTraining.h5\n",
      "Epoch 2/500\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.62834 to 0.55405, saving model to best_model_nnh2jobTraining.h5\n",
      "Epoch 3/500\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.55405 to 0.53134, saving model to best_model_nnh2jobTraining.h5\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.53134 to 0.52996, saving model to best_model_nnh2jobTraining.h5\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.52996 to 0.52595, saving model to best_model_nnh2jobTraining.h5\n",
      "Epoch 6/500\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.52595 to 0.52547, saving model to best_model_nnh2jobTraining.h5\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.52547\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.52547\n",
      "Epoch 9/500\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.52547 to 0.52488, saving model to best_model_nnh2jobTraining.h5\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.52488\n",
      "Epoch 11/500\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.52488\n",
      "Epoch 12/500\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.52488\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.52488\n",
      "Epoch 14/500\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.52488 to 0.52374, saving model to best_model_nnh2jobTraining.h5\n",
      "Epoch 15/500\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.52374 to 0.52311, saving model to best_model_nnh2jobTraining.h5\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.52311\n",
      "Epoch 17/500\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.52311\n",
      "Epoch 18/500\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.52311\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.52311\n",
      "Epoch 20/500\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.52311\n",
      "Epoch 21/500\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.52311\n",
      "Epoch 22/500\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.52311\n",
      "Epoch 23/500\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.52311\n",
      "Epoch 24/500\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.52311\n",
      "Epoch 25/500\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.52311\n",
      "Epoch 26/500\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.52311\n",
      "Epoch 27/500\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.52311\n",
      "Epoch 28/500\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.52311\n",
      "Epoch 29/500\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.52311\n",
      "Epoch 30/500\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.52311\n",
      "Epoch 31/500\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.52311\n",
      "Epoch 32/500\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.52311\n",
      "Epoch 33/500\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.52311\n",
      "Epoch 34/500\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.52311\n",
      "Epoch 35/500\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.52311\n",
      "Epoch 36/500\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.52311\n",
      "Epoch 37/500\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.52311\n",
      "Epoch 38/500\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.52311 to 0.52262, saving model to best_model_nnh2jobTraining.h5\n",
      "Epoch 39/500\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.52262\n",
      "Epoch 40/500\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.52262\n",
      "Epoch 41/500\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.52262\n",
      "Epoch 42/500\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.52262\n",
      "Epoch 43/500\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.52262 to 0.52224, saving model to best_model_nnh2jobTraining.h5\n",
      "Epoch 44/500\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.52224\n",
      "Epoch 45/500\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.52224\n",
      "Epoch 46/500\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.52224\n",
      "Epoch 47/500\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.52224\n",
      "Epoch 48/500\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.52224\n",
      "Epoch 49/500\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.52224\n",
      "Epoch 50/500\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.52224\n",
      "Epoch 51/500\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.52224\n",
      "Epoch 52/500\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.52224\n",
      "Epoch 53/500\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.52224\n",
      "Epoch 54/500\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.52224 to 0.52199, saving model to best_model_nnh2jobTraining.h5\n",
      "Epoch 55/500\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.52199 to 0.52130, saving model to best_model_nnh2jobTraining.h5\n",
      "Epoch 56/500\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.52130\n",
      "Epoch 57/500\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.52130 to 0.52118, saving model to best_model_nnh2jobTraining.h5\n",
      "Epoch 58/500\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.52118\n",
      "Epoch 59/500\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.52118\n",
      "Epoch 60/500\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.52118\n",
      "Epoch 61/500\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.52118\n",
      "Epoch 62/500\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.52118\n",
      "Epoch 63/500\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.52118\n",
      "Epoch 64/500\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.52118\n",
      "Epoch 65/500\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.52118\n",
      "Epoch 66/500\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.52118\n",
      "Epoch 67/500\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.52118\n",
      "Epoch 68/500\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.52118\n",
      "Epoch 69/500\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.52118\n",
      "Epoch 70/500\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.52118\n",
      "Epoch 71/500\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.52118\n",
      "Epoch 72/500\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.52118\n",
      "Epoch 73/500\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.52118\n",
      "Epoch 74/500\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.52118\n",
      "Epoch 75/500\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.52118\n",
      "Epoch 76/500\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.52118\n",
      "Epoch 77/500\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.52118\n",
      "Epoch 78/500\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.52118\n",
      "Epoch 79/500\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.52118\n",
      "Epoch 80/500\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.52118\n",
      "Epoch 81/500\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.52118\n",
      "Epoch 82/500\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.52118\n",
      "Epoch 00082: early stopping\n",
      "---- Train error ----\n",
      "0.13197765117490945\n",
      "---- CV error ----\n",
      "0.16837114132317604\n",
      "---- leaderboard stats ----\n",
      "0.02151234570725602\n",
      "0.19611902251454533\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Baseline test brier score = 0.18521499553665194\n",
      "Model test brier score = 0.17933373289374832\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------variable: layoff ---------------\n",
      "\n",
      "\n",
      "\n",
      "Gridsearch complete\n",
      "best params from gridsearch = {'dropout_layer_1': 0.95, 'learning_rate': 2.5e-05, 'num_hidden_layer_1': 256}\n",
      "Epoch 1/500\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63284, saving model to best_model_nnh2layoff.h5\n",
      "Epoch 2/500\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63284 to 0.57579, saving model to best_model_nnh2layoff.h5\n",
      "Epoch 3/500\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.57579 to 0.54052, saving model to best_model_nnh2layoff.h5\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.54052 to 0.52655, saving model to best_model_nnh2layoff.h5\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.52655 to 0.51799, saving model to best_model_nnh2layoff.h5\n",
      "Epoch 6/500\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.51799 to 0.51258, saving model to best_model_nnh2layoff.h5\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.51258 to 0.51186, saving model to best_model_nnh2layoff.h5\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.51186 to 0.51158, saving model to best_model_nnh2layoff.h5\n",
      "Epoch 9/500\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.51158 to 0.51148, saving model to best_model_nnh2layoff.h5\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.51148 to 0.51113, saving model to best_model_nnh2layoff.h5\n",
      "Epoch 11/500\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.51113 to 0.51079, saving model to best_model_nnh2layoff.h5\n",
      "Epoch 12/500\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.51079 to 0.51070, saving model to best_model_nnh2layoff.h5\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.51070\n",
      "Epoch 14/500\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.51070\n",
      "Epoch 15/500\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.51070\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.51070\n",
      "Epoch 17/500\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.51070\n",
      "Epoch 18/500\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.51070\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.51070\n",
      "Epoch 20/500\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.51070\n",
      "Epoch 21/500\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.51070\n",
      "Epoch 22/500\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.51070\n",
      "Epoch 23/500\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.51070 to 0.51000, saving model to best_model_nnh2layoff.h5\n",
      "Epoch 24/500\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.51000\n",
      "Epoch 25/500\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.51000\n",
      "Epoch 26/500\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.51000\n",
      "Epoch 27/500\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.51000\n",
      "Epoch 28/500\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.51000 to 0.50989, saving model to best_model_nnh2layoff.h5\n",
      "Epoch 29/500\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.50989\n",
      "Epoch 30/500\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.50989\n",
      "Epoch 31/500\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.50989\n",
      "Epoch 32/500\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.50989\n",
      "Epoch 33/500\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.50989\n",
      "Epoch 34/500\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.50989\n",
      "Epoch 35/500\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.50989 to 0.50974, saving model to best_model_nnh2layoff.h5\n",
      "Epoch 36/500\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.50974 to 0.50962, saving model to best_model_nnh2layoff.h5\n",
      "Epoch 37/500\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.50962 to 0.50902, saving model to best_model_nnh2layoff.h5\n",
      "Epoch 38/500\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.50902\n",
      "Epoch 39/500\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.50902\n",
      "Epoch 40/500\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.50902 to 0.50896, saving model to best_model_nnh2layoff.h5\n",
      "Epoch 41/500\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.50896 to 0.50862, saving model to best_model_nnh2layoff.h5\n",
      "Epoch 42/500\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.50862 to 0.50843, saving model to best_model_nnh2layoff.h5\n",
      "Epoch 43/500\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.50843\n",
      "Epoch 44/500\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.50843 to 0.50824, saving model to best_model_nnh2layoff.h5\n",
      "Epoch 45/500\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.50824\n",
      "Epoch 46/500\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.50824\n",
      "Epoch 47/500\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.50824\n",
      "Epoch 48/500\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.50824\n",
      "Epoch 49/500\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.50824 to 0.50817, saving model to best_model_nnh2layoff.h5\n",
      "Epoch 50/500\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.50817 to 0.50796, saving model to best_model_nnh2layoff.h5\n",
      "Epoch 51/500\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.50796\n",
      "Epoch 52/500\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.50796\n",
      "Epoch 53/500\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.50796 to 0.50795, saving model to best_model_nnh2layoff.h5\n",
      "Epoch 54/500\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.50795 to 0.50791, saving model to best_model_nnh2layoff.h5\n",
      "Epoch 55/500\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.50791\n",
      "Epoch 56/500\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.50791\n",
      "Epoch 57/500\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.50791 to 0.50754, saving model to best_model_nnh2layoff.h5\n",
      "Epoch 58/500\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.50754 to 0.50745, saving model to best_model_nnh2layoff.h5\n",
      "Epoch 59/500\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.50745\n",
      "Epoch 60/500\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.50745\n",
      "Epoch 61/500\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.50745\n",
      "Epoch 62/500\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.50745\n",
      "Epoch 63/500\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.50745\n",
      "Epoch 64/500\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.50745\n",
      "Epoch 65/500\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.50745\n",
      "Epoch 66/500\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.50745\n",
      "Epoch 67/500\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.50745\n",
      "Epoch 68/500\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.50745\n",
      "Epoch 69/500\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.50745\n",
      "Epoch 70/500\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.50745\n",
      "Epoch 71/500\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.50745\n",
      "Epoch 72/500\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.50745\n",
      "Epoch 73/500\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.50745\n",
      "Epoch 74/500\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.50745\n",
      "Epoch 75/500\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.50745\n",
      "Epoch 76/500\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.50745\n",
      "Epoch 77/500\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.50745\n",
      "Epoch 78/500\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.50745\n",
      "Epoch 79/500\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.50745\n",
      "Epoch 80/500\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.50745\n",
      "Epoch 81/500\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.50745\n",
      "Epoch 82/500\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.50745\n",
      "Epoch 00082: early stopping\n",
      "---- Train error ----\n",
      "0.13499425006273338\n",
      "---- CV error ----\n",
      "0.1521993647367975\n",
      "---- leaderboard stats ----\n",
      "0.004557843558553154\n",
      "0.173321753041633\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Baseline test brier score = 0.16721354282637468\n",
      "Model test brier score = 0.16579530664424838\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# variable = 'jobTraining'\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from keras.layers.core import Lambda\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.wrappers.scikit_learn import KerasClassifier,KerasRegressor\n",
    "from tensorflow.keras.metrics import MeanSquaredError\n",
    "from keras.regularizers import l1\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(0)\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(0)\n",
    "\n",
    "\n",
    "acc_nns = dict()\n",
    "best_params_dict = dict()\n",
    "train_cv_errors = dict()\n",
    "for variable in ['eviction','jobTraining','layoff']: # Looping through categorical outcomes\n",
    "    print('---------------variable:',variable,'---------------\\n\\n\\n')\n",
    "    # Get non-na rows for this specific variable\n",
    "    y_train_no_na = y_train[variable].dropna()\n",
    "    X_train_no_na = X_train.loc[y_train_no_na.index.values]\n",
    "\n",
    "    y_CV_no_na = y_CV.copy()[variable].dropna()\n",
    "    X_CV_no_na = X_CV.copy().loc[y_CV_no_na.index.values]\n",
    "\n",
    "    mask = leaderboard[variable].isna()\n",
    "    x_leaderboard = background_imputed_tot.loc[leaderboard.index.values]\n",
    "    y_leaderboard = leaderboard[variable]\n",
    "    x_leaderboard_no_na = x_leaderboard.loc[y_leaderboard.index]\n",
    "\n",
    "    y_train_trans= y_train_no_na.copy()\n",
    "    y_CV_trans = y_CV_no_na.copy()\n",
    "    y_test_trans = test[variable].dropna()\n",
    "\n",
    "\n",
    "    x_test_no_na = x_test.loc[y_test_trans.index.values]\n",
    "\n",
    "\n",
    "    ##### Normalize features to be between 0 and 1 and apply the same transformation on the validation and test sets\n",
    "    # Pandas applies these operations row-wise\n",
    "    X_train_normalized = X_train_no_na.copy()\n",
    "    X_cv_normalized = X_CV_no_na.copy()\n",
    "    x_leaderboard_normalized = x_leaderboard_no_na.copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ##### Normalize features to be between 0 and 1 and apply the same transformation on the validation and test sets\n",
    "    # Pandas applies these operations row-wise\n",
    "    X_train_normalized = X_train_no_na.copy()\n",
    "    X_cv_normalized = X_CV_no_na.copy()\n",
    "    x_leaderboard_normalized = x_leaderboard_no_na.copy()\n",
    "    x_test_normalized = x_test_no_na.copy()\n",
    "\n",
    "\n",
    "    column_min_normalization = X_train_no_na[continuous_cols_lst].min()\n",
    "    column_max_normalization = X_train_no_na[continuous_cols_lst].max()\n",
    "\n",
    "\n",
    "    X_train_normalized[continuous_cols_lst]=(X_train_no_na[continuous_cols_lst]-column_min_normalization)\\\n",
    "    /(column_max_normalization-column_min_normalization)\n",
    "\n",
    "    X_cv_normalized[continuous_cols_lst]=(X_CV_no_na[continuous_cols_lst]-column_min_normalization)\\\n",
    "    /(column_max_normalization-column_min_normalization)\n",
    "\n",
    "    x_leaderboard_normalized[continuous_cols_lst] = (x_leaderboard_no_na[continuous_cols_lst]-column_min_normalization)\\\n",
    "    /(column_max_normalization-column_min_normalization)\n",
    "\n",
    "    x_test_normalized[continuous_cols_lst] = (x_test_no_na[continuous_cols_lst]-column_min_normalization)\\\n",
    "    /(column_max_normalization-column_min_normalization)\n",
    "\n",
    "    classifier = KerasRegressor(make_classification_nn, epochs=500,verbose = 0)\n",
    "\n",
    "    params = [{'num_hidden_layer_1': [64,128,256],\n",
    "            'dropout_layer_1': [0.8,0.9,0.95,0.98],\n",
    "            'learning_rate': [0.0001,0.00005,0.000025]}]\n",
    "\n",
    "\n",
    "    # Use repeated stratified cross validation to preserve the class distributions\n",
    "    grid = GridSearchCV(classifier,\n",
    "                          param_grid=params,\n",
    "                          scoring='neg_mean_squared_error', #sklearn optimizing by maximizing negative brier score\n",
    "                          #  n_jobs=-1,\n",
    "                          verbose=0,\n",
    "                          cv=RepeatedStratifiedKFold(3,2)# Number of folds for CV\n",
    "                          #  fit_params={'callbacks': [tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                          #                                          min_delta=0.001, \n",
    "                          #                                          patience=25, \n",
    "                          #                                          mode='min',\n",
    "                          #                                          verbose=2)],\n",
    "                          #             'validation_data': (np.array(X_cv_normalized), np.array(y_CV_trans))\n",
    "                          #             }\n",
    "                    )\n",
    "\n",
    "    grid.fit(np.array(X_train_normalized), np.array(y_train_trans),callbacks= [tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                                  min_delta=0.001, \n",
    "                                                                  patience=25, \n",
    "                                                                  mode='min',\n",
    "                                                                  verbose=0)],validation_data= (np.array(X_cv_normalized), np.array(y_CV_trans)))\n",
    "\n",
    "\n",
    "    print('Gridsearch complete')\n",
    "    best_params = grid.best_params_\n",
    "\n",
    "    print('best params from gridsearch =',best_params)\n",
    "    best_params_dict[variable] = best_params\n",
    "    # model_test = make_model(best_params['num_hidden_layer_1'],best_params['dropout_layer_1'],best_params['learning_rate'])\n",
    "\n",
    "    num_hidden_layer_1 = best_params['num_hidden_layer_1']\n",
    "    dropout_layer_1 =best_params['dropout_layer_1']\n",
    "\n",
    "    learning_rate = best_params['learning_rate'] # jobTraining lr (also works well for layoff)\n",
    "    # learning_rate = 0.000060947\n",
    "\n",
    "     ###### Train the same neural network architecture from scratch (doesn't seem to work when I call my make regression function, so made it manually here)\n",
    "    model_test = Sequential()\n",
    "    # Defining input layer, with size of input feature (size of image is 784)\n",
    "    model_test.add(Dense(num_hidden_layer_1, input_shape=(len(X_train_normalized.columns),)))\n",
    "    model_test.add(Activation('sigmoid'))     \n",
    "    # Using a dropout rate of 20% for regularization for each layer\n",
    "    model_test.add(Dropout(dropout_layer_1))\n",
    "\n",
    "\n",
    "    # Defining third hidden layer\n",
    "    model_test.add(Dense(1))\n",
    "    model_test.add(Activation('sigmoid'))\n",
    "\n",
    "    model_test.compile(optimizer=Adam(\n",
    "              learning_rate = learning_rate),\n",
    "              loss='binary_crossentropy', metrics=['accuracy',MeanSquaredError()])#     model_test.summary()\n",
    "\n",
    "\n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                                  min_delta=0.0001, \n",
    "                                                                  patience=25, \n",
    "                                                                  verbose=2)\n",
    "    # Saving parameters at the earliest iteration when validation loss was at a minimum\n",
    "    mc = ModelCheckpoint('best_model_nnh2' + variable + '.h5', monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "    history = model_test.fit(X_train_normalized, y_train_trans, epochs=500, validation_split=0.2,\\\n",
    "                    validation_data=(X_cv_normalized, y_CV_trans),\\\n",
    "                      callbacks=[stop_early, mc], verbose = -1\n",
    "                    )\n",
    "\n",
    "    # model_test = load_model('best_model_nnh2' + variable + '.h5')\n",
    "\n",
    "    model_test.save('best_model_nn_overall_'+variable+'.h5')\n",
    "\n",
    "    y_pred_train = model_test.predict(X_train_normalized)\n",
    "    y_pred_all = model_test.predict(x_leaderboard_normalized)\n",
    "    cv_preds = model_test.predict(X_cv_normalized)\n",
    "    test_preds = model_test.predict(x_test_normalized)\n",
    "\n",
    "    # Can use mean square error as performance metric for both, since for 1 output, the mean square error and brier loss are the same.\n",
    "    print('---- Train error ----')\n",
    "    print(mean_squared_error(y_train_trans,y_pred_train))\n",
    "    print('---- CV error ----')\n",
    "    print(mean_squared_error(y_CV_trans,cv_preds))\n",
    "    train_cv_errors[variable] = [mean_squared_error(y_train_trans,y_pred_train),mean_squared_error(y_CV_trans,cv_preds)]\n",
    "\n",
    "    # y_CV_trans = (y_CV_no_na-y_train_mean)/y_train_std\n",
    "    print('---- leaderboard stats ----')\n",
    "    print(r2_score(y_leaderboard[~mask],y_pred_all[~mask]))\n",
    "    print(mean_squared_error(y_leaderboard[~mask],y_pred_all[~mask]))\n",
    "    print('\\n\\n\\n')\n",
    "    preds_baseline_test = np.array([y_test_trans.value_counts()[True]/(y_test_trans.value_counts()[False]+y_test_trans.value_counts()[True]) for i in range(y_test_trans.value_counts()[False]+y_test_trans.value_counts()[True])])\n",
    "\n",
    "    print('Baseline test brier score =',mean_squared_error(y_test_trans,preds_baseline_test))\n",
    "\n",
    "    print('Model test brier score =',mean_squared_error(test_preds,y_test_trans))\n",
    "    acc_nns[variable] = mean_squared_error(y_leaderboard[~mask],y_pred_all[~mask])\n",
    "    print('\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the best parameters and the model's training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bdyN5CvQ0VGe"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('best_params_classification_nn.pickle', 'wb') as handle:\n",
    "    pickle.dump(best_params_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "with open('history_accs_nn.pickle', 'wb') as handle:\n",
    "    pickle.dump(history_accs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Copy of Copy of May6 Current_NN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
